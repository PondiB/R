---
title: "Land Use Land Cover classification"
author: CIAT
date: "`r format(Sys.time(), '%d %B, %Y, %H:%M')`"
fontsize: 11pt
geometry: margin=1in
output:
  pdf_document:
    fig_caption: true
    fig_height: 5
    fig_width: 7
---

```{r echo=FALSE}
#Land Use Land Cover classification
#John Mutua, CIAT
```

### Objectives
This manual will help you conduct a land use land cover classification of a 
6-band Landsat 8 image. At the end of this session, you will be able to:

1.  Import a Landsat image into R 
2.  Import training data in the form of a shapefile into R
3.  Extract pixel data to train and fit a Random forests model
4.  Speed up image classification through parallel processing (Bonus)

For more details on source of Landsat data see 
(http://earthexplorer.usgs.gov/).

Before you start this session, it is important you have (i) the latest 
[R software](https://cran.r-project.org/bin/windows/base/) and (ii) 
[Rstudio](https://www.rstudio.com/) installed in your computer.

First, clear your workspace.
```{r}
#clear your workspace
rm(list = ls(all = TRUE))
```

```{r echo=FALSE}
library("knitr")
opts_knit$set(root.dir = "C:/LDN_Workshop/Sample_dataset/Land_Use_Land_Cover")
```

```{r setup, include=FALSE, cache=FALSE}
muffleError <- function(x,options) {}
knit_hooks$set(error=muffleError)
```

Set the start of spatial data processing.
```{r}
#set the start of spatial data processing
startTime <- Sys.time()
cat("Start time", format(startTime),"\n")
```

###Loading the data in R

Set your working directory. This is where you will save all outputs.
```{r}
#set your working directory
setwd("C:/LDN_Workshop/Sample_dataset/Land_Use_Land_Cover")
```

List down the packages to be used in this session. Packages will be installed 
if not already installed. They will then be loaded into the session.
```{r}
#load packages
.packages = c("rgdal","raster","caret")
.inst <- .packages %in% installed.packages()
if(length(.packages[!.inst]) > 0) install.packages(.packages[!.inst])
lapply(.packages, require, character.only=TRUE)
```

If you need to reproduce the results next time, set the seed. You can use any 
number.
```{r}
#set random seed
set.seed(322)
```

Import the image into R as a 'RasterBrick' object using the brick function from 
the 'raster' package. Also let's replace the original band names with shorter 
ones ( e.g. 'B1' to 'B7').
```{r}
#import the image into R
img <- brick("TOA/Otji_TOA.tif")
names(img) <- c(paste0("B", 2:7, coll = ""))
img
```

Before we begin the actual land cover classification, let's calculate NDVI and 
save the output for use in the next session. The resulting NDVI can be plotted 
as a map, values range from 0.2 (bare soils) to 0.6 (dense vegetation) in the 
area.
```{r}
#calculate NDVI and save the output for use in the next session
NDVI <- (img[[4]] - img[[3]]) / (img[[4]] + img[[3]])
writeRaster(NDVI, "Otji_NDVI.tif", overwrite=TRUE)
plot(NDVI, main="NDVI map - Otjiwarongo")
```

We can make a natural colour visualization of the Landsat image in R using the 
'plotRGB' command, for example, a natural colour composite 4:3:2  (Red - Green 
- Blue).We use the expression 'img * (img >= 0)' to convert the negative values 
to zero.
```{r}
#make a natural colour visualization of the Landsat image
plotRGB(img * (img >= 0), r = 4, g = 3, b = 2, scale = 10000)
```

Import the training data into R as an object of class 
'SpatialPolygonsDataFrame' and create a variable to store the name of the 
'class' column. Codes used in the training data include: 1-Forest, 2-Woodland, 
31-Bushland, 32-Grassland, 42-Cultivated area, 51-Wetland, 52-Water body, 
61-Artificial Surface, 71-Bareland, forest and Woodland classes later combined 
to Forest/woodland.
```{r}
#import the training data into R
trainData <- shapefile("Training_data/Otji_trainingData.shp")
responseCol <- "LC_Code"
```

The training dataset ('Otji_trainingData.shp') stores the ID for each land 
cover type in a column in the attribute table called ‘LC_Code’ as shown below:

![Training data as seen in ESRI ArcGIS](C:/LDN_Workshop/Sample_dataset/Land_Use_Land_Cover/figures/training_data_screenshot.png)


Plot the training data.
```{r}
#plot the training data.
plot(trainData, main="Distribution of training data", axes=FALSE)
```

###Extracting training pixels values

Extract the pixel values in the training areas for every band in the Landsat 
image and store them in a data frame along with the corresponding land cover 
class ID. The code below allows you to extract training data using both point 
and polygon shapefiles.
```{r}
#extract the pixel values in the training areas
trainSet = data.frame(matrix(vector(), nrow = 0, ncol = length(names(img)) + 1))
for (i in 1:length(unique(trainData[[responseCol]]))){
  category <- unique(trainData[[responseCol]])[i]
  categorymap <- trainData[trainData[[responseCol]] == category,]
  dataSet <- extract(img, categorymap)
  
  if(is(trainData, "SpatialPointsDataFrame")){
    dataSet <- cbind(dataSet, class = as.numeric(category))
    trainSet <- rbind(trainSet, dataSet)
  }
  if(is(trainData, "SpatialPolygonsDataFrame")){
    dataSet <- lapply(dataSet, function(x){cbind(x, class = 
                                                   as.numeric(rep(category, 
                                                                  nrow(x))))})
    df <- do.call("rbind", dataSet)
    trainSet <- rbind(trainSet, df)
  }
}
```

Partition the data into training and testing, this will enable us conduct 
accuracy tests.
```{r}
#partition the data into training and testing
inData <- createDataPartition(y = trainSet$class, p = 0.7, list = FALSE)
training <- trainSet[inData,]
testing <- trainSet[-inData,]
```

As you can see below, the training and testing 'data.frames' contains values 
for each of six 'Landsat' TOA bands plus the class attribute.
```{r}
#how does the class look like
table(training$class)
table(testing$class)
```

In our case, we will use 1000 observations which are randomly sample from the 
training data.frame to train the model.
```{r}
#randomly sample from the training data.frame
train_sample <- training[sample(1:nrow(training), 1000), ]
table(train_sample$class)
```

###Fitting the Random Forests model

Define and fit the .RandomForests. model using the 'train' function from the 
'caret' package by specifying the model as a formula with the dependent
variable (i.e., the land cover types IDs) encoded as factors.
```{r}
#fit the trandom forest model
rf_Otji <- train(as.factor(class) ~ B3 + B4 + B5, method = "rf", data =  
                   train_sample)
```

We can now use the 'predict' command to make a raster with predictions from the 
fitted model object (i.e., 'rf_Otji'). Speed up computations using the 
'clusterR' function from the 'raster' package which supports 
multi-core computing for functions such as predict (NB: install 'snow' package).
```{r}
#cluster the predictions
beginCluster()
prediction <- clusterR(img, raster::predict, args = list(model = rf_Otji))
endCluster()
```

Test the accuracy using the 'testing' dataset as it is an independent set 
of data and let's examine the producer's accuracy (aka sensitivity in the caret 
package) for the model.
```{r}
#test accuracy of testing dataset
prediction_2 <- predict(rf_Otji, testing)
confusionMatrix(prediction_2, testing$class)$overall[1]
confusionMatrix(prediction_2, testing$class)$byClass[, 1]
```

Save your classified image as a GeoTIFF.
```{r}
#save your classified image as a GeoTIFF
writeRaster(prediction, "Otji_classified.tif", overwrite=TRUE)
```

Visualize your classified image and add a legend to the plot. Can you recall 
this categories 1-Forest, 2-Woodland, 31-Bushland, 32-Grassland, 
42-Cultivated area, 51-Wetland, 52-Water body, 61-Artificial Surface, 
71-Bareland?
```{r}
#visualize your classified image
cols <- c("dark green", "forestgreen", "yellow", "magenta", "blue", "red")
plot(prediction, col=cols, legend=FALSE, main="Land Use Land Cover-Otjiwarongo")
legend("bottomright", 
       legend=c("Forest", "Woodland", "Bushland", "Grassland", "Water body", 
                "Bare land"), fill=cols, bg="white", 
       title = "Land cover 2016")
```

Finally, check the amount of time you spent conducting this analysis.
```{r}
#check the amount of time you spent conducting this analysis
timeDiff <- Sys.time() - startTime
cat("\nProcessing time", format(timeDiff), "\n")
```
